{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf', RandomForestClassifier()), ('svc', SVC())])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y= make_moons(n_samples=200, noise=0.15)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rndf_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "                estimators=[('lr', log_clf),\n",
    "                            ('rf', rndf_clf),\n",
    "                            ('svc', svm_clf)],\n",
    "                voting='hard'\n",
    ")\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.88\n",
      "RandomForestClassifier 0.94\n",
      "SVC 0.98\n",
      "VotingClassifier 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rndf_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging and Pasting in Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "            DecisionTreeClassifier(),\n",
    "            n_estimators=500,\n",
    "            max_samples=100,\n",
    "            bootstrap=True,          # Pasting instead; set False\n",
    "            n_jobs=-1,\n",
    ")\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out-of-Bag Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9266666666666666"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "            DecisionTreeClassifier(),\n",
    "            n_estimators=500,\n",
    "            max_samples=100,\n",
    "            bootstrap=True,\n",
    "            n_jobs=-1,\n",
    "            oob_score=True,\n",
    ")\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_                  # Likely to achieve 94.6% accuracy on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's Verify\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99609375, 0.00390625],\n",
       "       [0.07509881, 0.92490119],\n",
       "       [0.99215686, 0.00784314],\n",
       "       [0.96538462, 0.03461538],\n",
       "       [0.73809524, 0.26190476]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape is (150, 2) because X_test has 150 instances.\n",
    "bag_clf.oob_decision_function_[:5] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Patches and Random Subspaces\n",
    "\n",
    "1. **Random Patches**: Sampling both training instances and features.\n",
    "2. **Random Subspaces**: \n",
    "    \n",
    "    keeping all training instances (*`bootstrap = False`* and *`max_samples = 1.0`*)\n",
    "    \n",
    "    ***BUT*** \n",
    "    \n",
    "    sampling features (*`bootstrap_features = True`* and *`max_features < 1.0`*)\n",
    "    \n",
    "Sampling feature result in even more predictor diversity.\n",
    "- Trading a bit more bias for a lower variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests\n",
    "\n",
    "- Genrally trained via bagging method.\n",
    "- *`max_sample`* set to the size of training set.\n",
    "\n",
    "The following code uses all availabile CPU cores to train a Random Forest classifier with 500 trees (each limited to maximum of 16 leaf nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*RandomForestClassfier* has all the hyperparameters of a *DecisionTreeClassifier* and *BaggingeClassifier*.\n",
    "- Introduces extra randomness; Searches for very best feature among a random subset of feature.\n",
    "- Trades for higher bias for lower variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following BagClassifier is just as same as above RandomForestClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "            DecisionTreeClassifier(splitter='random', max_leaf_nodes=16),\n",
    "            n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra-trees | Extremely Randomized Trees Ensemble\n",
    "\n",
    "Using **random thresholds for each feature** rather than searching for the best possible thresholds (like Decsion Trees do)\n",
    "\n",
    "Use `ExtraTreesClassifier` class, `ExtraTreesRegressor`; both of their API identical to RandomForestClassifier/RandomForestRegressor\n",
    "- Trades more Bias for a lower variance\n",
    "- Much faster to train than regular Random Forests.\n",
    "\n",
    "  B/c finding the best possible threhold for each feature at every node is one the most time-consuming tasks of growing a tree\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "Scikit-Learn measures a feature's importance by looking at how much the tree nodes that use that feature reduce impurity on average (across all tree in the forest).\n",
    "\n",
    "Scikit-learn computes this score automatically for each feature after training, then it sacles the result so that the sum of all imporatances is equal to 1. \n",
    "- *`feature_importances_`* will provide the result.\n",
    "\n",
    "**Random Forests are very handy to get a quick understanding of what features actually matter**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.08475828580501725\n",
      "sepal width (cm) 0.022786389573764637\n",
      "petal length (cm) 0.4270325182779528\n",
      "petal width (cm) 0.46542280634326527\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris['data'], iris['target'])\n",
    "\n",
    "for name, score in zip(iris['feature_names'], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "**Refers to any ensemble method that can combine several weak learners into a strong learner.**\n",
    "- Train predictors sequentially, each trying to correct its predecessor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "**`AdaBoostClassifier`**\n",
    "\n",
    "New predictors focuses more and more on the hard cases, by paying more attention to the training instances the predecessor underfitted.\n",
    "\n",
    "- Alogrithm first trains a base classifier, uses it to make prediction, on the training set.\n",
    "- Algorithm then increases the relative weight of miscalssified training instances.\n",
    "- Train second classifier, using updated weights, and again makes predictions, update instance weights, and so on.\n",
    "\n",
    "*AdaBoost adds predictors to the ensemble, gradually making it better*; instead of tweaking single predictors's parameters to minimize a cost function, like Gradient Descent.\n",
    "\n",
    "**Can't be parallelized, since each predictor can only be trained after the previous predictor has been trained and evaluated**\n",
    "\n",
    "**IF OVERFITTING:**\n",
    "1. Reduce the number of estimators\n",
    "2. More strongly regularize the base estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=500)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), \n",
    "    n_estimators=500,\n",
    "    algorithm=\"SAMME.R\",\n",
    "    learning_rate=0.5)\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "**`GradientBoostingRegressor`** much like `RandomForestRegressor`, has hyperparameters to control the growth of Decision Trees, as well as hyperparameter to control the ensemble training (such as *`n_estimaotors`*) \n",
    "\n",
    "Just like AdaBoost, but instead of tweaking the instance weights at every instance, this method \n",
    "\n",
    "### Tries to fit the new predictor to the *residual errors* made by the previous predictor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we'll train a second DecisionTreeRegressor on the residual errors made by the first predictor.\n",
    "y2 = y - tree_reg1.predict(X)\n",
    "\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we'll train a third DecisonTreeRegeressor on the residual errors made by the second predictor.\n",
    "y3 = y2 - tree_reg2.predict(X)\n",
    "\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3.fit(X, y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we have ensemble of three trees.\n",
    "- **Predictions will be made by simply adding up the the predictions of all the trees.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\n",
    "y_pred "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Sklearn class\n",
    "\n",
    "*`learning_rate`* hyperparameter scales the contribution of each tree.\n",
    "\n",
    "- If set to low value (like `0.1`) you'll need more trees in the ensemble to fit the training set.\n",
    "\n",
    "  **But will generalize better**. This is a regularization technique called *shrienkage*.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
    "gbrt.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to find optimal number of trees, use **Early Stopping**.\n",
    "The below code trains 120 trees first and then looks back for the number of tree at which the validation error (mean_squared_error) was least.\n",
    "\n",
    "`staged_predict()` method returns an iterator over the prediction made by the ensemble at each stage of training (like ensemble with one tree, two tree, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "errors = [mean_squared_error(y_val, y_pred) for y_pred in gbrt.staged_predict(X_val)]\n",
    "best_number_of_estimators = np.argmin(errors) + 1\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=best_number_of_estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actually stopping early\n",
    "By setting *`warm_start`*`=True` makes Sckit-learn keep existing trees when `fit()` method is called, allowing **incremental training**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stops training if the validation error does not improve for five iterations in a row.\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\n",
    "\n",
    "min_val_error = float('inf')\n",
    "error_going_up = 0\n",
    "\n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    val_error = mean_squared_error(y_val, gbrt.predict(X_val))\n",
    "    \n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:\n",
    "            break             # EARLY STOPPING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stocashastic Gradient Boosting\n",
    "*`subsample`* hyperparameter specifies the fraction of training instances to be used for training each tree.\n",
    "\n",
    "- Trades a higher bias for lower variance.\n",
    "- Speeds up training considerably.\n",
    "- *`subsample`*`=0.25`, then each tree is trained on 25% of the training instances, selected RANDOMLY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-5e4aaaf9815e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mxgb_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mxgb_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "\n",
    "xgb_reg = xgboost.XGBRegressor()\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "y_pred = xgb_reg.predict(X_val)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost also offers several nice features, such as automatically taking care of early stopping\n",
    "\n",
    "xgb_reg.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=2)\n",
    "xgb_reg.predict(X_val)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking | Stacked Generalization\n",
    "Instead of using a trivial function (like hard voting) to perform the aggregate the predictions of all predictors,\n",
    "\n",
    "### Train a model to perform this aggregation itself.\n",
    "- To train a blender, a common approach is to use a hold-out set.\n",
    "- Split a training set into two, first for training predictors in first layer and second (Hold-out).\n",
    "- The first layer predictors are made to make predictions on the hold-out set.\n",
    "- each predictors will output its predictions and this becomes the training set for the blender.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
