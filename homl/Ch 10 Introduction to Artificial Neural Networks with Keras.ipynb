{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Perceptron\n",
    "The perceptron exhibits a different artficial neuron called <mark>*Threshold logic unit* (TLU)</mark>, or sometime a <mark>*Linear Threshold Unit* (LTU)</mark>. The inputs and outputs are numbers. The TLU coputes a weighted sum of its inputs ($z=w_1x_1+w_2x_2+...+w_nx_n = x^Tw)$, then applies a <mark>*step function*</mark> to that sum and outputs the result: $h_w(x) = step(z)$ where $z = x^Tw$.\n",
    "\n",
    "The most common step function used in Perceptron is **Heaviside step function** while somethimes sign function is also preferred.\n",
    "$$\\text{heaviside}\\,(z)\\> = \\> \\begin{cases} 0 \\>\\text{if} \\>z<0 \\\\ 1 \\>\\text{if} \\>z >=0 \\end{cases}  \\qquad \\text{sgn}\\, (z)\\> = \\> \\begin{cases} -1 \\>\\text{if} \\> z<0 \\\\ 0 \\quad\\text{if}\\> z=0 \\\\ +1 \\> \\text{if}\\> z>0 \\end{cases} $$\n",
    "\n",
    "A perceptron is simply composed of a single layer of TLUs.<mark> When all the neurons in a layer are connected to every neuron in the <b>previous layer</b>, the layer is called a **fully connected layer**</mark> or a *dense layer*. The inputs of the perceptron are fed to special passthrough neurons called *input neurons*, and all these input neurons form *input layer*. An extra bias feature is generally added ($x_0\\>=\\>1)$ represented by *bias neuron*, which outputs 1 all the time.\n",
    "\n",
    "Computing output of a fully connected layer.\n",
    "- $h_{W,\\, b} = \\Phi\\>(XW\\, + \\, b)$\n",
    "\n",
    "where,\n",
    "- $X$ represents the matrix of input feature. <mark>One row per instance</mark> and <mark>one column for per feature.</mark>\n",
    "- Weight Matrix $W$ except for the one from the bias neuron. <mark>One row per neuron</mark> and <mark>One column per artificial neuron in the layer.</mark>\n",
    "- Bias vector $b$ contains all the connection weights between the bias neuron and the artificial neurons. <mark>One bias term per artificial neuron.</mark>\n",
    "- Activation function $\\Phi$: when the artificial neurons are TLUs, it is a *step function*.\n",
    "\n",
    "### Training Algorithm \n",
    "*Hebb's rule* \"Cells that fire togther, wire together\"; the connection weight between two neurons tends to increase when they fire simultaneously.\n",
    "\n",
    "Perceptron training is also done in the same that resembles the above mentioned rule. For every output neuron that produced a wrong perdiction, it reinforces the connection weights from the inputs that would have contributed to the correct prediction.\n",
    "\n",
    "Perceptron learning rule (weight update):\n",
    "- $w_{i,\\,j}^{(\\text{next}\\;\\text{step})}\\> = \\> w_{i,\\,j} \\> +\\> \\eta\\big(y_j\\, -\\,\\hat{y_j}\\big)x_i$\n",
    "\n",
    "where,\n",
    "\n",
    "- $w_{i,\\,j}$ is the <mark>connection weight between the $i^{\\text{th}}$ input neuron and  $j^{\\text{th}}$ output neuron.</mark>\n",
    "- $\\eta$ is <mark>learning rate.</mark>\n",
    "- $x_i$ is  <mark>$i^{\\text{th}}$ input value of the current training instance.</mark>\n",
    "- $y_j$ is the <mark>target output of the  <b>$j^{\\text{th}}$ output neuron</b></mark> for the current training instance.\n",
    "- $\\hat{y_j}$ is the <mark>output of the  $j^{\\text{th}}$ output neuron</mark> for the current training instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()0lllo\n",
    "X = iris.data[:, (2, 3)] # petal length, petal width\n",
    "y = (iris.target == 0).astype(np.int) # Iris setosa\n",
    "\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5]])\n",
    "y_predj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptrons do not output a class probability; rather, they make predictions based on a hard threshold. o"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
