{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Perceptron\n",
    "The perceptron exhibits a different artficial neuron called <mark>*Threshold logic unit* (TLU)</mark>, or sometime a <mark>*Linear Threshold Unit* (LTU)</mark>. The inputs and outputs are numbers. The TLU coputes a weighted sum of its inputs ($z=w_1x_1+w_2x_2+...+w_nx_n = x^Tw)$, then applies a <mark>*step function*</mark> to that sum and outputs the result: $h_w(x) = step(z)$ where $z = x^Tw$.\n",
    "\n",
    "The most common step function used in Perceptron is **Heaviside step function** while somethimes sign function is also preferred.\n",
    "$$\\text{heaviside}\\,(z)\\> = \\> \\begin{cases} 0 \\>\\text{if} \\>z<0 \\\\ 1 \\>\\text{if} \\>z >=0 \\end{cases}  \\qquad \\text{sgn}\\, (z)\\> = \\> \\begin{cases} -1 \\>\\text{if} \\> z<0 \\\\ 0 \\quad\\text{if}\\> z=0 \\\\ +1 \\> \\text{if}\\> z>0 \\end{cases} $$\n",
    "\n",
    "A perceptron is simply composed of a single layer of TLUs.<mark> When all the neurons in a layer are connected to every neuron in the <b>previous layer</b>, the layer is called a **fully connected layer**</mark> or a *dense layer*. The inputs of the perceptron are fed to special passthrough neurons called *input neurons*, and all these input neurons form *input layer*. An extra bias feature is generally added ($x_0\\>=\\>1)$ represented by *bias neuron*, which outputs 1 all the time.\n",
    "\n",
    "Computing output of a fully connected layer.\n",
    "- $h_{W,\\, b} = \\Phi\\>(XW\\, + \\, b)$\n",
    "\n",
    "where,\n",
    "- $X$ represents the matrix of input feature. <mark>One row per instance</mark> and <mark>one column for per feature.</mark>\n",
    "- Weight Matrix $W$ except for the one from the bias neuron. <mark>One row per neuron</mark> and <mark>One column per artificial neuron in the layer.</mark>\n",
    "- Bias vector $b$ contains all the connection weights between the bias neuron and the artificial neurons. <mark>One bias term per artificial neuron.</mark>\n",
    "- Activation function $\\Phi$: when the artificial neurons are TLUs, it is a *step function*.\n",
    "\n",
    "### Training Algorithm \n",
    "*Hebb's rule* \"Cells that fire togther, wire together\"; the connection weight between two neurons tends to increase when they fire simultaneously.\n",
    "\n",
    "Perceptron training is also done in the same that resembles the above mentioned rule. For every output neuron that produced a wrong perdiction, it reinforces the connection weights from the inputs that would have contributed to the correct prediction.\n",
    "\n",
    "Perceptron learning rule (weight update):\n",
    "- $w_{i,\\,j}^{(\\text{next}\\;\\text{step})}\\> = \\> w_{i,\\,j} \\> +\\> \\eta\\big(y_j\\, -\\,\\hat{y_j}\\big)x_i$\n",
    "\n",
    "where,\n",
    "\n",
    "- $w_{i,\\,j}$ is the <mark>connection weight between the $i^{\\text{th}}$ input neuron and  $j^{\\text{th}}$ output neuron.</mark>\n",
    "- $\\eta$ is <mark>learning rate.</mark>\n",
    "- $x_i$ is  <mark>$i^{\\text{th}}$ input value of the current training instance.</mark>\n",
    "- $y_j$ is the <mark>target output of the  <b>$j^{\\text{th}}$ output neuron</b></mark> for the current training instance.\n",
    "- $\\hat{y_j}$ is the <mark>output of the  $j^{\\text{th}}$ output neuron</mark> for the current training instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)] # petal length, petal width\n",
    "y = (iris.target == 0).astype(np.int) # Iris setosa\n",
    "\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5]])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptrons do not output a class probability; rather, they make predictions based on a hard threshold. But the fact that they are incapable of solving some trivail problems like the *Exclusive OR* (XOR) classification problem make them little less promising. Well, this problem and bunch of other can be just solved for Perceptrons by stacking up layers of Perceptrons. This will result in what is called as <mark>*Multilayer Perceptron* (MLP)</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Multilayer Perceptron and Backpropagation\n",
    "An MLP is composed for variety of things which can be named are:\n",
    "- ***input layer***: one passthrough layer\n",
    "- ***hidden layer***: one or more layers of TLUs, called hidden because their \"values\" are not given in the data.\n",
    "- ***output layer**: Final layer of TLUs.\n",
    "- ***lower layers**: Layers closer to the input layer\n",
    "- ***upper layers**: layers closer to the output layer\n",
    "\n",
    "Every layer except the output layer includes a bias neuron and is fully connected to the next layer.\n",
    "\n",
    "> The signal flows only in one direction, so this architecture is an example of a <mark>*feedforward neural network* (FNN)</mark>.\n",
    "\n",
    "When an ANN contains a deep stack of hideen layers, it is referred to as <mark>*deep neural network* (DNN)</mark>.\n",
    "\n",
    "# Backpropagation\n",
    "It is the most popular algorithm for training MLPs and other DNN. In short, it is Gradient Descent using an efficient technique for computing the gradients mathematically.\n",
    "\n",
    "> Automatically computing gradients is called <mark>*automatic differentiation*</mark> or <mark>*autodiff*</mark>. The one used by backpropagation is called <mark>*reverse mode autodiff*</mark>. It is fast and well suited when the function to differentiate has many variables (connection weights) and few outputs(one loss).\n",
    "\n",
    "**Let's see through each step**:\n",
    "- It handles one mini-batch at a time (for example, containing 32 instances each) and it goes through the full training set multiple time, Each pass is called an <mark>*epoch*</mark>\n",
    "- Each mini-batch is passed to the network's input layer. <mark>*Forward pass*</mark>: it is exactly like making predictions, except all intermediate results are preserved since they are needed for the backward pass.\n",
    "- <mark>Next, the algorithm measures the network;s output error.</mark>\n",
    "- <mark>Compute how much each output cnnection contributed to the error, by applying *chain rule*.</mark>\n",
    "- <mark>Then measure how much of these error contribution came from each connection in the layer below, again using chain rule,</mark> working backward until the algorithm reaches the input layer. This reverse pass efficiently measures the error gradient across all the connection weights in the network by <mark> propagating the error gradient backward through the network</mark>. (Hence the name).\n",
    "\n",
    "> ⚠ It is important to initialize all the hidden layers' connection weights randomly, or lese the training will fail.\n",
    "\n",
    "A key change was also applied: replaing the activation function from sigmoid function to RELU. Because the step function contains only flat segments, so there is no gradient for gradient descent to go around. There are other choices too:\n",
    "- <mark>*Hyperbolic Tangent function*</mark>: $$tanh\\,(z) \\> = \\> 2\\sigma(2z)\\,-\\,1$$.\n",
    "    \n",
    "  The output value ranges from -1 to 1. That range tends to make each layer's output more or less centered around 0 at beginning of training, which often helps speed up convergence.\n",
    "\n",
    "- <mark>*Rectified Linear Unit Function*</mark>: $$ReLU\\,(z) \\> = \\> max(0, z)$$\n",
    "\n",
    "    Continuous but unfortunately not differentaible at $z$ = 0. Derivative is 0 for $z$ <0. In practice however,it works very well. Also, the fact that it does not have a maximum output value helps reduce some issues during Gradient Descent. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression MLPs\n",
    "When building an MLP for regression, we do not want to use any activation function for the ouptput neurons. \n",
    "If in need of \n",
    "- **Output to be always positive**: \n",
    "    \n",
    "    Then you can use the ReLU activation function in the output layer. \n",
    "    \n",
    "    Alternatively we can also use <mark>*softplus* activation function: $\\text{softplus}\\,(z) \\> = \\> \\log(1+ \\exp(z))$</mark>, which is close to 0 when $z$ is negative, and close to $z$ when $z$ is positive. \n",
    "- **Output to Fall within certain range**:\n",
    "    \n",
    "    You can use logistic function OR the hyperbolic tangent, and then scale the labels to the appropriate range: 0 to 1 for the logistic function and -1 to 1 for the hyperbolic tangent. \n",
    "    \n",
    "The loss function is generally mse; but if training set has lot of outliers, prefer mean absolute error instead. Alternativelym we can also use Huber loss, which is a combination of both.\n",
    "\n",
    "> ✅ The huber loss is quadratic when the error is smaller than a threshold $\\delta$ (typically 1) but linear when the error is larger than $\\delta$. <mark>The quadratic part allows it to converge faster and be more precise than the mean absolute error</mark>, and <mark>the linear part makes it less sensitive to outliers than the mean squared error.</mark>\n",
    "\n",
    "#### Typical regression MLP architecture\n",
    "| Hyperparameter             | Typical value                                                        |\n",
    "| ---                        | ---                                                                  |\n",
    "| **# input neurons**            | One per input feature                                                |\n",
    "| **# hidden layers**          | Depends on the problem, but typically 1 to 5                         |\n",
    "| **# neurons per hidden layer** | Depends on the problem, but typically 10 to 100                      |\n",
    "| **# output neurons**           | 1 per prediction dimension                                           |\n",
    "| **Hidden activation**          | ReLU (or SELU)                                                       |\n",
    "| **Output Activation**          | None, or ReLU (positive outputs) or logitic/tanh (if bounded outputs)|\n",
    "| **Loss Function**              | MSE or MAE/Huber (if outliers)                                       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification MLPs\n",
    "Single output neuron using logistic function, the output (between 0 and 1) can be interpret as probability of the positive class. The probability of negative class will be 1 minus the output. \n",
    "\n",
    "MLPS can also easily handle multilabel binary classification tasks. You will need as many output neurons as there are labels.\n",
    "\n",
    "MLPs are also vaible to do multiclass classification. You need to have one output neuron per class and then we just need to <mark>put a softmax function for the whole output layer.</mark>. The softmax function will ensure that all the estimated probabilities lies between 0 and 1 and they add up to 1 (which is required if the classes are exclusive). \n",
    "\n",
    "Typical classification MLP architecture:\n",
    "\n",
    "| Hyperparameter | Binary classification | Mulitlabel binary classification | Multiclass Classification |\n",
    "| --- | --- | --- | --- |\n",
    "| **Input and hidden layers**| same as regression | same as regression | same as regression |\n",
    "| **# output neurons** | 1 | 1 per label | 1 per class |\n",
    "| **Output layer activation** | Logistic | Logistic | Softmax |\n",
    "| **Loss function** | Cross Entropy | Cross Entropy | Cross Entropy |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing MLPs with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an Image Classifier Using the Sequential API\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
