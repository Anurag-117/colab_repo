{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Componenet Analysis (PCA)\n",
    "The following code uses Numpy's `svd()` function to obtain all the principal components of the training set, then extract the two unit vectors that define the first two PCs.\n",
    "**Computational Complexity** = $O(m \\times n^2) + O(n^3)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_centered = X - X.mean(axis=0)\n",
    "U, s, VT = np.linalg.svd(X_centered)\n",
    "c1 = VT.T[:, 0]\n",
    "c2 = Vt.T[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To project the training set onto the hyperplane\n",
    "$$X_{d\\_proj} = XW_d$$\n",
    "where:\n",
    "- $X_{d\\_proj}$ reduced dataset of dimensionality $d$\n",
    "- $X$ training set matrix\n",
    "- $W_d$ matrix containing the first $d$ columns of $V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wd = VT.T[:, :2]\n",
    "X_2d = X_centered.dot(Wd)  # X2d i.e. X is now reduced to 2 dimensional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_componenents=2)\n",
    "X_2d = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting `PCA` transformer; `components_` attribute holds the **transpose** of $W_d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first principle componenet\n",
    "pca.componenets.T[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explained variance ratio\n",
    "`PCA().explained_variance_ratio_`\n",
    "#### Indicates the proportion to the dataset's variance that lies along each principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explined_variance_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Right Number of Dimensions\n",
    "#### Choose the number of dimensions that add up to suffieciently large portion of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "cumsum = np.cumsum(pca.explaines_variance_ratio)\n",
    "req_dimensions = np.argmax(cumsum) + 1          # index start at '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yet BETTER way to do the this\n",
    "Instead of setting `n_compnents`= req_dimension`,\n",
    "\n",
    "Put the `n_components` a float no between 0.0 and 1.0, for the amount of variance you want to preserve(typically 95% for next process OR 2-3 for DataViz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another option is to plot explained variance ratio as a function of the number of dimensions; plot `cumsum`\n",
    "There ususally be an elbow  in the curve, where the explained variance stops growing fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA for Compression\n",
    "To recover from data from compression: `PCA().inverse_transform` \n",
    "\n",
    "$$X_{recovered} = X_{d\\_proj}{W_d}^T$$\n",
    "The mean squared distance between the original data and the reconstructed data is called **reconstruction error**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_componenets=154)\n",
    "X_reduced = pca.fit_transform(X_train)\n",
    "X_recovered = pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized PCA\n",
    "`svd_solver = 'randomized'`\n",
    "\n",
    "Has a reduced complexity of the first *d* principle components from full SVD approach of $O(m \\times n^2) + O(n^3)$.\n",
    "\n",
    "**Computational Complexity** = $O(m \\times d^2) + O(d^2)$\n",
    "\n",
    "- When $d$ is much smaller than $m$.\n",
    "\n",
    "By default `svd_solver = 'auto'`: Automatically use randomized PCA **IF**\n",
    "- $m$ or $n$ is greater than 500 **AND** $d$ is less than 80% of $m$ or $n$. \n",
    "- or else use full SVD approach\n",
    "\n",
    "Set `svd_solver = 'full'` to force use of full SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_pca = PCA(n_components=154, svd_solver='randomized')\n",
    "X_reduced = rnd_pca.fit_tranform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental PCA (IPCA)\n",
    "`IncrementalPCA`\n",
    "\n",
    "Allows training set to be split into mini-batches and feed an IPCA algorithm one mini-batch at a time.\n",
    "- Use `partial_fit` rather than `fit`.\n",
    "\n",
    "### Using `np.arrary_split()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "n_batches =  100\n",
    "inc_pca = IncrementalPCA(n_components=154)\n",
    "\n",
    "for X_batch in np.array_split(X_train, n_batches):\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "    \n",
    "n_reduced = inc_pca.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `np.memmap()`\n",
    "**The class only loads the data it needs in memory, when it needs it.** Which allows you to manipulate a large array stored in a binary file on disk as if it were entirely in memory.\n",
    "\n",
    "This maked it possible t call the usual `fit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mm = np.memmap(filename, dtype='float32', mode='readonly', shape=(m, n))\n",
    "\n",
    "batch_size = m // n_batches\n",
    "inc_pca = IncrementalPCA(n_componenets=154, batch_ssize=batch_size)\n",
    "inc_pca.fit(X_mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel PCA\n",
    "An unsupervised learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "rbf_pca = kernelPCA(n_components=2, kernel='rbf', gamma=0.04)\n",
    "X_reduced = rbf_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting a Kernel and Tuning Hyperparameters\n",
    "Since kPCA is an unsupervised algorithm, there is no obvious performance to help you select the best kernel and hyperparameter value. But since dimensionality reduction is often preparation step, after which the reduced dataset is send to supervised algorithm, \n",
    "#### we can measure ML model performance and selct the best hyperparameter values for the kPCA.\n",
    "\n",
    "The following code creates a two step pipeline, first reducing dimensionality to two dimensions using kPCA, then applying Logistic Regression for classification. Then it uses GridSearchCV to find the best kernel and gamma values for kPCA in order to get the best calssification accuracy at the end of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('kpca', KernelPCA(n_components=2)),\n",
    "    ('log_reg', LogisticRegression())\n",
    "])\n",
    "param_grid = [{\n",
    "    'kpca_gamma': np.linspace(0.83, 0.05, 10),\n",
    "    'kpca_kernel': ['rbf', 'sigmoid']\n",
    "}]\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another approach is to select the kernel and hyperparameters that yield the lowest reconstruction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_pca = KernelPCA(n_components=2, kernel='rbf', gamma=0.0433,\n",
    "                    fit_inverse=True)\n",
    "X_reduced = rbf_pca.fit_transform(X)\n",
    "X_preimage = rbf_pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then compute the recontruction pre-image error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(X, X_preimage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use grid search with cross calidation to find the kernel and hyperparameters that minimize this error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locally Linear Embedding (LLE)\n",
    "- another powerful **Nonlinear dimensionality reduction** (NLDR) technique.\n",
    "- <mark>Manifold Learning technique</mark> that does not rely on projections, like the previous algorithm do.\n",
    "- Particularly good at unrolling twisted manifolds.\n",
    "\n",
    "In a nutshell, LLE works by first measuring how each training instance linearly relates to its closest neighbors (c.n.), and then looking for a low-dimensional representation of the training set where these local relationships are best preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\n",
    "X_reduced = lle.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How LLE works\n",
    "### Step one: Linearly modeling local relationships\n",
    "- For each training instance $x^{(i)}$, the algorithm identifies its $k$ closest neighbors (here $k$=10).\n",
    "- Then it tries to reconstruct $x^{(i)}$ as linear function of these neighbors.\n",
    "  \n",
    "  It finds the weights $w_{i,j}$ such that the squared distance between $x^{(i)}$ and $\\sum^{m}_{j=1}w_{i,j} x^{(j)}$ is as small as possible, assuming $w_{i,j}=0$ if $x^{(i)}$ is not one of the $k$ closest neighbors.\n",
    "  \n",
    "### Step two: Reducing dimensionality while preserving relationships\n",
    "Map the training instances into a $d$-dimensional space (where $d$ < $n$) while preserving these local relationships.\n",
    "- If $z^{(i)}$ is the image of $x^{(i)}$ in this $d$-dimensional space, then we want the squared distance between $z^{(i)}$ and $\\sum^{m}_{j=1}\\hat{w}_{i, j}z^{(i)}$ to be as small as possible.\n",
    "- Keeping the weights fixed and finding the optimal position of the instance's image in the low dimensional space.\n",
    "\n",
    "#### Computational Complexity     \n",
    "- For finding the k nearest neighbors = $O(mlog(m)nlog(k)$\n",
    "- For optimizing the weights = $O(mnk^3)$\n",
    "- For constructing the low dimensional representations = $O(dm^2)$\n",
    "\n",
    "The $m^2$ in the last term makes this algorithm scale poorly to very large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
