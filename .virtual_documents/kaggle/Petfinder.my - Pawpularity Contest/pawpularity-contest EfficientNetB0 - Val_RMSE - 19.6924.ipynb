import os
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import warnings 
import multiprocessing

warnings.filterwarnings('ignore')


TRAIN_PATH = "../input/petfinder-pawpularity-score/train/"
TEST_PATH = "../input/petfinder-pawpularity-score/test/"
ROOT_LOGDIR = os.path.join(os.curdir, "logs")

IMAGE_SIZE = 300
BATCH_SIZE = 50
NO_OF_EPOCHS = 100


train_df = pd.read_csv("../input/petfinder-pawpularity-score/train.csv")
test_df = pd.read_csv("../input/petfinder-pawpularity-score/test.csv")

# Adding paths for images
train_df["Path"] = TRAIN_PATH + train_df["Id"] + ".jpg"
test_df["Path"] = TEST_PATH + test_df["Id"] + ".jpg"
train_df.head()


test_df.head()


print("Training set instances:", train_df.shape[0])
print("Testing set instances:", test_df.shape[0])


def generate_image_dataset(filepath, label=None):
    image = tf.io.read_file(filepath)
    image = tf.image.decode_jpeg(image, channels=3) 
    iamge = tf.cast(image, tf.float32) / 255.0
    image = tf.image.resize(image, (IMAGE_SIZE, IMAGE_SIZE))
    return image, label


train, valid = train_test_split(train_df, test_size=0.1)
dataset_train = (tf.data.Dataset.from_tensor_slices((train["Path"].values,
                                                    train["Pawpularity"].values )).
                 map(generate_image_dataset).batch(BATCH_SIZE).prefetch(1))
dataset_valid = (tf.data.Dataset.from_tensor_slices((valid["Path"].values,
                                                    valid["Pawpularity"].values)).
                map(generate_image_dataset).batch(BATCH_SIZE).prefetch(1))


data_augmentation = keras.Sequential([
        keras.layers.experimental.preprocessing.Normalization(),
        keras.layers.experimental.preprocessing.Resizing(IMAGE_SIZE, IMAGE_SIZE),
        keras.layers.experimental.preprocessing.RandomFlip("horizontal"),
        # random rotation in range [-20% * 2pi, 20% * 2pi]
        keras.layers.experimental.preprocessing.RandomRotation(factor=0.02), 
        # A positive value means zooming out, while a negative value means zooming in.
        keras.layers.experimental.preprocessing.RandomZoom(
            height_factor=0.2, width_factor=0.2
        # output zoomed out vertically in range [20%, 20%]
        # output zoomed out horizontally in the range [20%, 20%]
        )
    ],
    name="data_augmentaion"
)


def get_run_logdir():
    import time
    run_id = time.strftime("run_%Y_%m_%d-%H_%M_%S")
    return os.path.join(ROOT_LOGDIR, run_id)


def train_experimental_model(model, optimizer,
                             checkpoint_name=None):
    model.compile(
        optimizer=optimizer,
        loss=keras.losses.MeanSquaredError(),
        metrics=[
            keras.metrics.RootMeanSquaredError(name="rmse"),
            "mae",
        ]
    )
    
    # Create callbacks
    if checkpoint_name==None:
        raise ValueError("Provide checpoint model name.")
    checkpoint_callback = keras.callbacks.ModelCheckpoint(
        checkpoint_name,
        monitor="val_rmse",
        save_best_only=True,
        save_weights_only=False
    )
    early_stopping_callback = keras.callbacks.EarlyStopping(
        patience=10,
        restore_best_weights=True,
        min_delta=1e-4,
    )
    reduce_lr = keras.callbacks.ReduceLROnPlateau(
        monitor="val_rmse",
        factor=0.3,
        patience=3,
        min_lr=1e-7
    )
    run_logdir = get_run_logdir()
    tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)
    
    history = model.fit(
        dataset_train,
        validation_data=dataset_valid,
        batch_size=BATCH_SIZE,
        epochs=NO_OF_EPOCHS,
        callbacks=[checkpoint_callback, early_stopping_callback, reduce_lr, tensorboard_cb]
    )
    return history


inputs = keras.layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))
efficient_net = keras.applications.EfficientNetB0(
    include_top=False,
    weights=None,
    input_tensor=inputs,
    input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),
    pooling=None,
    classes=None,
    classifier_activation=None
)
efficient_net_output = efficient_net(inputs)
# augmented_input = data_augmentation(efficient_net_output)
globalAvgPool2D = keras.layers.GlobalAvgPool2D()(efficient_net_output)
dense_layer_1 = keras.layers.Dense(100, activation="relu")(globalAvgPool2D)
dense_layer_2 = keras.layers.Dense(30, activation="relu")(dense_layer_1)
dense_layer_out = keras.layers.Dense(1)(dense_layer_2)

efficientNet_model = keras.Model(inputs=inputs, outputs=dense_layer_out)


efficientNet_model.summary()


history = train_experimental_model(efficientNet_model,optimizer=keras.optimizers.Adam(),
                                   checkpoint_name="efficientNetB0_model.h5")


# TODO: Comment/Delete this when commiting
# import os
# import multiprocessing

# !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip
# !unzip ngrok-stable-linux-amd64.zip

# pool = multiprocessing.Pool(processes = 10)
# results_of_processes = [pool.apply_async(os.system, args=(cmd, ), callback = None )
#                         for cmd in [
#                         f"tensorboard --logdir ./logs/ --host 0.0.0.0 --port 6006 &",
#                         "./ngrok http 6006 &"
#                         ]]

# ! curl -s http://localhost:4040/api/tunnels | python3 -c \
#     "import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])"


def generate_test_dataset(filepath):
    image = tf.io.read_file(filepath)
    image = tf.image.decode_jpeg(image, channels=3) 
    iamge = tf.cast(image, tf.float32) / 255.0
    image = tf.image.resize(image, (IMAGE_SIZE, IMAGE_SIZE))
    return image, 0


def generate_submission_csv(model):
    dataset_test = (tf.data.Dataset.from_tensor_slices(test_df["Path"].values).
                    map(generate_test_dataset).batch(BATCH_SIZE).prefetch(1))
    submission_df = test_df[["Id"]]
    submission_df = submission_df.assign(Pawpularity=model.predict(dataset_test).reshape(-1))
    submission_df.to_csv("submission.csv", index=False)
    
generate_submission_csv(efficientNet_model)



