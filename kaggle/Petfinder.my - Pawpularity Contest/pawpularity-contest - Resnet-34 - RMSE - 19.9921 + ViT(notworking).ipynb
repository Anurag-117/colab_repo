{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Petfinder.my - Pawpularity Contest\n",
    "Predict the popularity of shelter pet photos\n",
    "<img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/25383/logos/header.png\"></img>\n",
    "\n",
    "Analyze raw images and metadata to predict the “Pawpularity” of pet photos. The Pawpularity Score is derived from each pet profile's page view statistics at the listing pages, using an algorithm that normalizes the traffic data across different pages, platforms (web & mobile) and various metrics.\n",
    "\n",
    "## Photo Metadata\n",
    "The train.csv and test.csv files contain metadata for photos in the training set and test set, respectively. Each pet photo is labeled with the value of 1 (Yes) or 0 (No) for each of the following features:\n",
    "\n",
    "- Focus - Pet stands out against uncluttered background, not too close / far.\n",
    "- Eyes - Both eyes are facing front or near-front, with at least 1 eye / pupil decently clear.\n",
    "- Face - Decently clear face, facing front or near-front.\n",
    "- Near - Single pet taking up significant portion of photo (roughly over 50% of photo width or height).\n",
    "- Action - Pet in the middle of an action (e.g., jumping).\n",
    "- Accessory - Accompanying physical or digital accessory / prop (i.e. toy, digital sticker), excluding collar and leash.\n",
    "- Group - More than 1 pet in the photo.\n",
    "- Collage - Digitally-retouched photo (i.e. with digital photo frame, combination of multiple photos).\n",
    "- Human - Human in the photo.\n",
    "- Occlusion - Specific undesirable objects blocking part of the pet (i.e. human, cage or fence). Note that not all blocking objects are considered occlusion.\n",
    "- Info - Custom-added text or labels (i.e. pet name, description).\n",
    "- Blur - Noticeably out of focus or noisy, especially for the pet’s eyes and face. For Blur entries, “Eyes” column is always set to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T13:55:34.136125Z",
     "iopub.status.busy": "2021-09-27T13:55:34.135353Z",
     "iopub.status.idle": "2021-09-27T13:55:41.469371Z",
     "shell.execute_reply": "2021-09-27T13:55:41.468575Z",
     "shell.execute_reply.started": "2021-09-27T13:55:34.136083Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T13:55:41.471674Z",
     "iopub.status.busy": "2021-09-27T13:55:41.47113Z",
     "iopub.status.idle": "2021-09-27T13:55:41.477595Z",
     "shell.execute_reply": "2021-09-27T13:55:41.476932Z",
     "shell.execute_reply.started": "2021-09-27T13:55:41.471628Z"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"../input/petfinder-pawpularity-score/train/\"\n",
    "TEST_PATH = \"../input/petfinder-pawpularity-score/test/\"\n",
    "\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 100\n",
    "NO_OF_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T13:55:41.479352Z",
     "iopub.status.busy": "2021-09-27T13:55:41.478616Z",
     "iopub.status.idle": "2021-09-27T13:55:41.621405Z",
     "shell.execute_reply": "2021-09-27T13:55:41.620295Z",
     "shell.execute_reply.started": "2021-09-27T13:55:41.479301Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/petfinder-pawpularity-score/train.csv\")\n",
    "test_df = pd.read_csv(\"../input/petfinder-pawpularity-score/test.csv\")\n",
    "\n",
    "# Adding paths for images\n",
    "train_df[\"Path\"] = TRAIN_PATH + train_df[\"Id\"] + \".jpg\"\n",
    "test_df[\"Path\"] = TEST_PATH + test_df[\"Id\"] + \".jpg\"\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T13:55:41.623968Z",
     "iopub.status.busy": "2021-09-27T13:55:41.62354Z",
     "iopub.status.idle": "2021-09-27T13:55:41.640901Z",
     "shell.execute_reply": "2021-09-27T13:55:41.639992Z",
     "shell.execute_reply.started": "2021-09-27T13:55:41.623931Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T13:55:41.64235Z",
     "iopub.status.busy": "2021-09-27T13:55:41.642088Z",
     "iopub.status.idle": "2021-09-27T13:55:41.654975Z",
     "shell.execute_reply": "2021-09-27T13:55:41.653741Z",
     "shell.execute_reply.started": "2021-09-27T13:55:41.642322Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Training set instances:\", train_df.shape[0])\n",
    "print(\"Testing set instances:\", test_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating keras Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T14:30:17.613319Z",
     "iopub.status.busy": "2021-09-27T14:30:17.612271Z",
     "iopub.status.idle": "2021-09-27T14:30:17.713569Z",
     "shell.execute_reply": "2021-09-27T14:30:17.712172Z",
     "shell.execute_reply.started": "2021-09-27T14:30:17.613212Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_image_dataset(filepath, label=None):\n",
    "    image = tf.io.read_file(filepath)\n",
    "    image = tf.image.decode_jpeg(image, channels=3) \n",
    "    iamge = tf.cast(image, tf.float32) / 255.0\n",
    "    image = tf.image.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "    return image, label\n",
    "\n",
    "\n",
    "train, valid = train_test_split(train_df, test_size=0.1)\n",
    "dataset_train = (tf.data.Dataset.from_tensor_slices((train[\"Path\"].values,\n",
    "                                                    train[\"Pawpularity\"].values )).\n",
    "                 map(generate_image_dataset).batch(BATCH_SIZE).prefetch(1))\n",
    "dataset_valid = (tf.data.Dataset.from_tensor_slices((valid[\"Path\"].values,\n",
    "                                                    valid[\"Pawpularity\"].values)).\n",
    "                map(generate_image_dataset).batch(BATCH_SIZE).prefetch(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T13:55:41.876171Z",
     "iopub.status.busy": "2021-09-27T13:55:41.875527Z",
     "iopub.status.idle": "2021-09-27T13:55:41.924309Z",
     "shell.execute_reply": "2021-09-27T13:55:41.923295Z",
     "shell.execute_reply.started": "2021-09-27T13:55:41.876132Z"
    }
   },
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential([\n",
    "        keras.layers.experimental.preprocessing.Normalization(),\n",
    "        keras.layers.experimental.preprocessing.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "        # random rotation in range [-20% * 2pi, 20% * 2pi]\n",
    "        keras.layers.experimental.preprocessing.RandomRotation(factor=0.02), \n",
    "        # A positive value means zooming out, while a negative value means zooming in.\n",
    "        keras.layers.experimental.preprocessing.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        # output zoomed out vertically in range [20%, 20%]\n",
    "        # output zoomed out horizontally in the range [20%, 20%]\n",
    "        )\n",
    "    ],\n",
    "    name=\"data_augmentaion\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T13:55:47.737865Z",
     "iopub.status.busy": "2021-09-27T13:55:47.737519Z",
     "iopub.status.idle": "2021-09-27T13:55:47.748091Z",
     "shell.execute_reply": "2021-09-27T13:55:47.747031Z",
     "shell.execute_reply.started": "2021-09-27T13:55:47.737829Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_experimental_model(model, optimizer,\n",
    "                             checkpoint_name=None):\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.MeanSquaredError(),\n",
    "        metrics=[\n",
    "            keras.metrics.RootMeanSquaredError(name=\"rmse\"),\n",
    "            \"mae\",\n",
    "        ]\n",
    "    )\n",
    "    if checkpoint_name==None:\n",
    "        raise ValueError(\"Provide checpoint model name.\")\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_name,\n",
    "        monitor=\"val_rmse\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True\n",
    "    )\n",
    "    early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        dataset_train,\n",
    "        validation_data=dataset_valid,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=NO_OF_EPOCHS,\n",
    "        callbacks=[checkpoint_callback, early_stopping_callback]\n",
    "    )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's train a ResNet-34 CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T09:00:23.959449Z",
     "iopub.status.busy": "2021-09-27T09:00:23.959189Z",
     "iopub.status.idle": "2021-09-27T09:00:24.694947Z",
     "shell.execute_reply": "2021-09-27T09:00:24.694186Z",
     "shell.execute_reply.started": "2021-09-27T09:00:23.959417Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResidualUnit(keras.layers.Layer):\n",
    "    def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        self.main_layers = [\n",
    "            keras.layers.Conv2D(filters, 3, strides=strides, \n",
    "                                padding=\"same\", use_bias=False),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            self.activation,\n",
    "            keras.layers.Conv2D(filters, 3, strides=1, padding=\"same\",\n",
    "                               use_bias=False),\n",
    "            keras.layers.BatchNormalization()\n",
    "        ]\n",
    "        self.skip_layers = []\n",
    "        if strides > 1:\n",
    "            self.skip_layers = [\n",
    "                keras.layers.Conv2D(filters, 1, strides=strides,\n",
    "                                    padding=\"same\", use_bias=False),\n",
    "                keras.layers.BatchNormalization()\n",
    "            ]\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"activation\": self.activation,\n",
    "                \"main_layers\": self.main_layers,\n",
    "                \"skip_layers\": self.skip_layers}\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.main_layers:\n",
    "            Z = layer(Z)\n",
    "        skip_Z = inputs\n",
    "        for layer in self.skip_layers:\n",
    "            skip_Z = layer(skip_Z)\n",
    "        return self.activation(Z+skip_Z)\n",
    "    \n",
    "\n",
    "resnet_model = keras.models.Sequential()\n",
    "resnet_model.add(keras.layers.Conv2D(64, 7, strides=2, input_shape=[224, 224, 3],\n",
    "                             padding=\"same\", use_bias=False))\n",
    "resnet_model.add(keras.layers.BatchNormalization())\n",
    "resnet_model.add(keras.layers.Activation(\"relu\"))\n",
    "resnet_model.add(keras.layers.MaxPool2D(pool_size=2, strides=2, padding=\"same\"))\n",
    "prev_filters = 64\n",
    "for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n",
    "    strides = 1 if filters == prev_filters else 2\n",
    "    resnet_model.add(ResidualUnit(filters, strides=strides))\n",
    "    prev_filters = filters\n",
    "resnet_model.add(keras.layers.GlobalAvgPool2D())\n",
    "resnet_model.add(keras.layers.Flatten())\n",
    "resnet_model.add(keras.layers.Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train_experimental_model(resnet_model,optimizer=keras.optimizers.Adam(),\n",
    "                                   checkpoint_name=\"resnet_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's give Vision-Transformer a try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T13:59:45.059004Z",
     "iopub.status.busy": "2021-09-27T13:59:45.058695Z",
     "iopub.status.idle": "2021-09-27T13:59:45.064577Z",
     "shell.execute_reply": "2021-09-27T13:59:45.063977Z",
     "shell.execute_reply.started": "2021-09-27T13:59:45.058967Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001      \n",
    "patch_size = 16         # P: size of the patches to be extracted from the input images\n",
    "num_patches = (IMAGE_SIZE // IMAGE_SIZE) ** 2 # N = HW/P^2; considering H=W=S, \n",
    "                                             # then formula becomes N = (S/P)^2\n",
    "\n",
    "projection_dim = 64    # The dimension a patch will be projected into\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim\n",
    "]                             # Size of transformer layers\n",
    "transformer_layers = 8        # Lx\n",
    "mlp_head_units = [2048, 1024] # Size of the dense layers of the final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T13:59:45.36316Z",
     "iopub.status.busy": "2021-09-27T13:59:45.362735Z",
     "iopub.status.idle": "2021-09-27T13:59:45.376108Z",
     "shell.execute_reply": "2021-09-27T13:59:45.375162Z",
     "shell.execute_reply.started": "2021-09-27T13:59:45.363124Z"
    }
   },
   "outputs": [],
   "source": [
    "class Patches(keras.layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,                                    # A 4-D tensor\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],   # The size of the extracted patches\n",
    "            strides=[1, self.patch_size, self.patch_size, 1], # How far the centers of two consecutive patches are in the images\n",
    "            rates=[1, 1, 1, 1],                               # This is the input stride, specifying how far two\n",
    "                                                              # consecutive patch samples are in the input. \n",
    "            padding=\"VALID\"\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        # print(f\"Patches shape: {patches.shape}\")\n",
    "        # print(f\"Patches reshaped shape: {tf.reshape(patches, [batch_size, -1, patch_dims]).shape}\")\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"patch_size\": self.patch_size}\n",
    "    \n",
    "\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = keras.layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = keras.layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T13:59:45.720924Z",
     "iopub.status.busy": "2021-09-27T13:59:45.720598Z",
     "iopub.status.idle": "2021-09-27T13:59:45.72987Z",
     "shell.execute_reply": "2021-09-27T13:59:45.728706Z",
     "shell.execute_reply.started": "2021-09-27T13:59:45.720891Z"
    }
   },
   "outputs": [],
   "source": [
    "class PatchEncoder(keras.layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = keras.layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = keras.layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"num_patches\": self.num_patches,\n",
    "                \"projection\": self.projection}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T14:07:31.16058Z",
     "iopub.status.busy": "2021-09-27T14:07:31.159565Z",
     "iopub.status.idle": "2021-09-27T14:07:32.798936Z",
     "shell.execute_reply": "2021-09-27T14:07:32.797083Z",
     "shell.execute_reply.started": "2021-09-27T14:07:31.160521Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_vit_regressor():\n",
    "    inputs = keras.layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    # Augment data\n",
    "#     augmented_input = data_augmentation(inputs)\n",
    "    # Create patches\n",
    "    patches = Patches(patch_size)(inputs)\n",
    "    # Encode patches to add positional embedding to the projected patch\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block (Lx)\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1\n",
    "        x1 = keras.layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer\n",
    "        attention_output = keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection\n",
    "        x2 = keras.layers.Add()([attention_output, encoded_patches])\n",
    "        \n",
    "        # Layer normalization 2\n",
    "        x3 = keras.layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2\n",
    "        encoded_patches = keras.layers.Add()([x3, x2])\n",
    "    \n",
    "    # Create a [batch_size, projection_dim] tensor\n",
    "    representation = keras.layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = keras.layers.Flatten()(representation)\n",
    "    representation = keras.layers.Dropout(0.5)(representation)\n",
    "    # Add MLP\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    output = keras.layers.Dense(units=1)(features)\n",
    "    # Create the keras model\n",
    "    model = keras.Model(inputs=inputs, outputs=output)\n",
    "    return model\n",
    "\n",
    "vit_regressor = create_vit_regressor()\n",
    "history = train_experimental_model(vit_regressor,optimizer=keras.optimizers.Adam(),\n",
    "                                   checkpoint_name=\"vit_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T09:48:41.26302Z",
     "iopub.status.busy": "2021-09-27T09:48:41.262754Z",
     "iopub.status.idle": "2021-09-27T09:48:41.920212Z",
     "shell.execute_reply": "2021-09-27T09:48:41.919463Z",
     "shell.execute_reply.started": "2021-09-27T09:48:41.262992Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_test_dataset(filepath):\n",
    "    image = tf.io.read_file(filepath)\n",
    "    image = tf.image.decode_jpeg(image, channels=3) \n",
    "    iamge = tf.cast(image, tf.float32) / 255.0\n",
    "    image = tf.image.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "    return image, 0\n",
    "\n",
    "\n",
    "def generate_submission_csv(model):\n",
    "    dataset_test = (tf.data.Dataset.from_tensor_slices(test_df[\"Path\"].values).\n",
    "                    map(generate_test_dataset).batch(BATCH_SIZE).prefetch(1))\n",
    "    submission_df = test_df[[\"Id\"]]\n",
    "    submission_df = submission_df.assign(Pawpularity=model.predict(dataset_test).reshape(-1))\n",
    "    submission_df.to_csv(\"submission.csv\", index=False)\n",
    "    \n",
    "generate_submission_csv(resnet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
