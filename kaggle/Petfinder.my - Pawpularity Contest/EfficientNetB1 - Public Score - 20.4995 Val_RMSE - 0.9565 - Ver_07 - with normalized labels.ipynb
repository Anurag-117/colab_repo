{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Petfinder.my - Pawpularity Contest\nPredict the popularity of shelter pet photos\n<img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/25383/logos/header.png\"></img>\n\nAnalyze raw images and metadata to predict the “Pawpularity” of pet photos. The Pawpularity Score is derived from each pet profile's page view statistics at the listing pages, using an algorithm that normalizes the traffic data across different pages, platforms (web & mobile) and various metrics.\n\n## Photo Metadata\nThe train.csv and test.csv files contain metadata for photos in the training set and test set, respectively. Each pet photo is labeled with the value of 1 (Yes) or 0 (No) for each of the following features:\n\n- Focus - Pet stands out against uncluttered background, not too close / far.\n- Eyes - Both eyes are facing front or near-front, with at least 1 eye / pupil decently clear.\n- Face - Decently clear face, facing front or near-front.\n- Near - Single pet taking up significant portion of photo (roughly over 50% of photo width or height).\n- Action - Pet in the middle of an action (e.g., jumping).\n- Accessory - Accompanying physical or digital accessory / prop (i.e. toy, digital sticker), excluding collar and leash.\n- Group - More than 1 pet in the photo.\n- Collage - Digitally-retouched photo (i.e. with digital photo frame, combination of multiple photos).\n- Human - Human in the photo.\n- Occlusion - Specific undesirable objects blocking part of the pet (i.e. human, cage or fence). Note that not all blocking objects are considered occlusion.\n- Info - Custom-added text or labels (i.e. pet name, description).\n- Blur - Noticeably out of focus or noisy, especially for the pet’s eyes and face. For Blur entries, “Eyes” column is always set to 0.","metadata":{}},{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport warnings \nimport multiprocessing\n\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-09-29T06:28:38.899647Z","iopub.execute_input":"2021-09-29T06:28:38.900041Z","iopub.status.idle":"2021-09-29T06:28:44.086846Z","shell.execute_reply.started":"2021-09-29T06:28:38.899935Z","shell.execute_reply":"2021-09-29T06:28:44.086155Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"TRAIN_PATH = \"../input/petfinder-pawpularity-score/train/\"\nTEST_PATH = \"../input/petfinder-pawpularity-score/test/\"\nROOT_LOGDIR = os.path.join(os.curdir, \"logs\")\n\nIMAGE_SIZE = 224\nBATCH_SIZE = 30\nNO_OF_EPOCHS = 100\n\ninput_cols = ['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n              'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur']","metadata":{"execution":{"iopub.status.busy":"2021-09-29T06:29:07.550031Z","iopub.execute_input":"2021-09-29T06:29:07.550569Z","iopub.status.idle":"2021-09-29T06:29:07.555444Z","shell.execute_reply.started":"2021-09-29T06:29:07.550535Z","shell.execute_reply":"2021-09-29T06:29:07.554791Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/petfinder-pawpularity-score/train.csv\")\ntest_df = pd.read_csv(\"../input/petfinder-pawpularity-score/test.csv\")\n\n# Adding paths for images\ntrain_df[\"Path\"] = TRAIN_PATH + train_df[\"Id\"] + \".jpg\"\ntest_df[\"Path\"] = TEST_PATH + test_df[\"Id\"] + \".jpg\"\n\n# Normalize the labels\nscaler = StandardScaler()\ntrain_df[\"Pawpularity\"] = scaler.fit_transform(train_df[[\"Pawpularity\"]])\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-29T06:32:24.506118Z","iopub.execute_input":"2021-09-29T06:32:24.506400Z","iopub.status.idle":"2021-09-29T06:32:24.554492Z","shell.execute_reply.started":"2021-09-29T06:32:24.506371Z","shell.execute_reply":"2021-09-29T06:32:24.553817Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-09-29T06:30:41.326051Z","iopub.execute_input":"2021-09-29T06:30:41.326316Z","iopub.status.idle":"2021-09-29T06:30:41.340399Z","shell.execute_reply.started":"2021-09-29T06:30:41.326290Z","shell.execute_reply":"2021-09-29T06:30:41.339592Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-29T05:32:53.141921Z","iopub.execute_input":"2021-09-29T05:32:53.142568Z","iopub.status.idle":"2021-09-29T05:32:53.156440Z","shell.execute_reply.started":"2021-09-29T05:32:53.142533Z","shell.execute_reply":"2021-09-29T05:32:53.155514Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"print(\"Training set instances:\", train_df.shape[0])\nprint(\"Testing set instances:\", test_df.shape[0])","metadata":{"execution":{"iopub.status.busy":"2021-09-29T06:32:52.586833Z","iopub.execute_input":"2021-09-29T06:32:52.587491Z","iopub.status.idle":"2021-09-29T06:32:52.595909Z","shell.execute_reply.started":"2021-09-29T06:32:52.587458Z","shell.execute_reply":"2021-09-29T06:32:52.595180Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Creating keras Dataset","metadata":{}},{"cell_type":"code","source":"def generate_image_dataset(input_data, label):\n    filepath, tabular_cols = input_data\n    image = tf.io.read_file(filepath)\n    image = tf.image.decode_jpeg(image, channels=3) \n    iamge = tf.cast(image, tf.float32) / 255.0\n    image = tf.image.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n    return (image, tabular_cols), label\n\n\ntrain, valid = train_test_split(train_df, test_size=0.1)\ndataset_train = (tf.data.Dataset.from_tensor_slices(((train[\"Path\"].values, train[input_cols].values),\n                                                    train[\"Pawpularity\"].values )).\n                 map(generate_image_dataset).batch(BATCH_SIZE).prefetch(1))\ndataset_valid = (tf.data.Dataset.from_tensor_slices(((valid[\"Path\"].values, valid[input_cols].values),\n                                                    valid[\"Pawpularity\"].values)).\n                map(generate_image_dataset).batch(BATCH_SIZE).prefetch(1))","metadata":{"execution":{"iopub.status.busy":"2021-09-29T06:32:54.839933Z","iopub.execute_input":"2021-09-29T06:32:54.840417Z","iopub.status.idle":"2021-09-29T06:32:56.757190Z","shell.execute_reply.started":"2021-09-29T06:32:54.840387Z","shell.execute_reply":"2021-09-29T06:32:56.756473Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"data_augmentation = keras.Sequential([\n        keras.layers.experimental.preprocessing.Normalization(),\n        keras.layers.experimental.preprocessing.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n        keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n        # random rotation in range [-20% * 2pi, 20% * 2pi]\n        keras.layers.experimental.preprocessing.RandomRotation(factor=0.02), \n        # A positive value means zooming out, while a negative value means zooming in.\n        keras.layers.experimental.preprocessing.RandomZoom(\n            height_factor=0.2, width_factor=0.2\n        # output zoomed out vertically in range [20%, 20%]\n        # output zoomed out horizontally in the range [20%, 20%]\n        )\n    ],\n    name=\"data_augmentaion\"\n)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T06:32:59.552700Z","iopub.execute_input":"2021-09-29T06:32:59.553426Z","iopub.status.idle":"2021-09-29T06:32:59.873772Z","shell.execute_reply.started":"2021-09-29T06:32:59.553392Z","shell.execute_reply":"2021-09-29T06:32:59.873055Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def get_run_logdir():\n    import time\n    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n    return os.path.join(ROOT_LOGDIR, run_id)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T06:33:00.701674Z","iopub.execute_input":"2021-09-29T06:33:00.702357Z","iopub.status.idle":"2021-09-29T06:33:00.706404Z","shell.execute_reply.started":"2021-09-29T06:33:00.702324Z","shell.execute_reply":"2021-09-29T06:33:00.705458Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def train_experimental_model(model, optimizer,\n                             checkpoint_name=None):\n    model.compile(\n        optimizer=optimizer,\n        loss=keras.losses.MeanSquaredError(),\n        metrics=[\n            keras.metrics.RootMeanSquaredError(name=\"rmse\"),\n            \"mae\",\n        ]\n    )\n    \n    # Create callbacks\n    if checkpoint_name==None:\n        raise ValueError(\"Provide checpoint model name.\")\n    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n        checkpoint_name,\n        monitor=\"val_rmse\",\n        save_best_only=True,\n        save_weights_only=False\n    )\n    early_stopping_callback = keras.callbacks.EarlyStopping(\n        patience=10,\n        restore_best_weights=True,\n        min_delta=1e-4,\n    )\n    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n        monitor=\"val_rmse\",\n        factor=0.3,\n        patience=3,\n        min_lr=1e-7\n    )\n    run_logdir = get_run_logdir()\n    tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n    \n    history = model.fit(\n        dataset_train,\n        validation_data=dataset_valid,\n        batch_size=BATCH_SIZE,\n        epochs=NO_OF_EPOCHS,\n        callbacks=[checkpoint_callback, early_stopping_callback, reduce_lr, tensorboard_cb]\n    )\n    return history","metadata":{"execution":{"iopub.status.busy":"2021-09-29T06:38:35.901419Z","iopub.execute_input":"2021-09-29T06:38:35.901738Z","iopub.status.idle":"2021-09-29T06:38:35.914546Z","shell.execute_reply.started":"2021-09-29T06:38:35.901686Z","shell.execute_reply":"2021-09-29T06:38:35.913737Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# Let's train a EfficientNet CNN","metadata":{}},{"cell_type":"code","source":"inputs_image = keras.layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\ninputs_tabular = keras.layers.Input(shape=(len(input_cols)))\n\ntabular_dense = keras.layers.Dense(16, activation=\"relu\")(inputs_tabular)\ntabular_dense = keras.layers.Dropout(rate=0.4)(tabular_dense)\ntabular_dense = keras.layers.Dense(16, activation=\"relu\")(tabular_dense)\ntabular_dense = keras.layers.Dropout(rate=0.4)(tabular_dense)\ntabular_dense = keras.layers.Dense(16, activation=\"relu\", \n                                   kernel_regularizer=keras.regularizers.l2(0.01))(tabular_dense)\ntabular_dense = keras.layers.Dropout(rate=0.4)(tabular_dense)\n\naugmented_image = data_augmentation(inputs_image)\nefficient_net = keras.applications.EfficientNetB1(\n    include_top=False,\n    weights=None,\n    input_tensor=augmented_image,\n    input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n    pooling=None,\n    classes=None,\n    classifier_activation=None\n)\nefficient_net_output = efficient_net(augmented_image)\nglobalAvgPool2D = keras.layers.GlobalAvgPool2D()(efficient_net_output)\ndense_layer_1 = keras.layers.Dense(100, activation=\"relu\")(globalAvgPool2D)\ndense_layer_1 = keras.layers.Dropout(rate=0.4)(dense_layer_1)\ndense_layer_2 = keras.layers.Dense(30, activation=\"relu\")(dense_layer_1)\ndense_layer_2 = keras.layers.Dropout(rate=0.4)(dense_layer_2)\n\nconcatenate = keras.layers.Concatenate(axis=1)([dense_layer_2, tabular_dense])\ndense_layer_out = keras.layers.Dense(1, \n                                     kernel_regularizer=keras.regularizers.l2(0.01))(concatenate)\n\nefficientNet_model = keras.Model(inputs=(inputs_image, inputs_tabular), outputs=dense_layer_out)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T06:38:37.231812Z","iopub.execute_input":"2021-09-29T06:38:37.232491Z","iopub.status.idle":"2021-09-29T06:38:40.136308Z","shell.execute_reply.started":"2021-09-29T06:38:37.232455Z","shell.execute_reply":"2021-09-29T06:38:40.135543Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"efficientNet_model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-09-29T06:38:40.137886Z","iopub.execute_input":"2021-09-29T06:38:40.138125Z","iopub.status.idle":"2021-09-29T06:38:40.175079Z","shell.execute_reply.started":"2021-09-29T06:38:40.138089Z","shell.execute_reply":"2021-09-29T06:38:40.174402Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"history = train_experimental_model(efficientNet_model, optimizer=keras.optimizers.Adam(),\n                                   checkpoint_name=\"efficientNetB1_model.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-09-29T06:40:35.791095Z","iopub.execute_input":"2021-09-29T06:40:35.791411Z","iopub.status.idle":"2021-09-29T07:24:36.144448Z","shell.execute_reply.started":"2021-09-29T06:40:35.791373Z","shell.execute_reply":"2021-09-29T07:24:36.143659Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"# Visualization on Tensorboard","metadata":{}},{"cell_type":"code","source":"# TODO: Comment/Delete this Before commiting\n# import os\n# import multiprocessing\n\n# # !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n# # !unzip ngrok-stable-linux-amd64.zip\n\n# pool = multiprocessing.Pool(processes = 10)\n# results_of_processes = [pool.apply_async(os.system, args=(cmd, ), callback = None )\n#                         for cmd in [\n#                         f\"tensorboard --logdir ./logs/ --host 0.0.0.0 --port 6006 &\",\n#                         \"./ngrok http 6006 &\"\n#                         ]]\n\n# ! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n#     \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"","metadata":{"execution":{"iopub.status.busy":"2021-09-29T06:40:13.119075Z","iopub.execute_input":"2021-09-29T06:40:13.121380Z","iopub.status.idle":"2021-09-29T06:40:15.203257Z","shell.execute_reply.started":"2021-09-29T06:40:13.121318Z","shell.execute_reply":"2021-09-29T06:40:15.202090Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"# Submission ","metadata":{}},{"cell_type":"code","source":"def generate_test_dataset(filepath, tabular_cols):\n    image = tf.io.read_file(filepath)\n    image = tf.image.decode_jpeg(image, channels=3) \n    iamge = tf.cast(image, tf.float32) / 255.0\n    image = tf.image.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n    return (image, tabular_cols), 0\n\n\ndef generate_submission_csv(model):\n    dataset_test = (tf.data.Dataset.from_tensor_slices((test_df[\"Path\"].values, test_df[input_cols].values)).\n                    map(generate_test_dataset).batch(BATCH_SIZE).prefetch(1))\n    submission_df = test_df[[\"Id\"]]\n    submission_df = submission_df.assign(Pawpularity=scaler.inverse_transform(model.predict(dataset_test).reshape(-1)))\n    submission_df.to_csv(\"submission.csv\", index=False)\n    \ngenerate_submission_csv(efficientNet_model)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T07:35:24.716817Z","iopub.execute_input":"2021-09-29T07:35:24.717655Z","iopub.status.idle":"2021-09-29T07:35:25.028153Z","shell.execute_reply.started":"2021-09-29T07:35:24.717610Z","shell.execute_reply":"2021-09-29T07:35:25.027363Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}