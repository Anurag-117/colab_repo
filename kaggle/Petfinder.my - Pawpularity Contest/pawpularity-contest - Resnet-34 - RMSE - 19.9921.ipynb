{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# Petfinder.my - Pawpularity Contest\n","Predict the popularity of shelter pet photos\n","<img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/25383/logos/header.png\"></img>\n","\n","Analyze raw images and metadata to predict the “Pawpularity” of pet photos. The Pawpularity Score is derived from each pet profile's page view statistics at the listing pages, using an algorithm that normalizes the traffic data across different pages, platforms (web & mobile) and various metrics.\n","\n","## Photo Metadata\n","The train.csv and test.csv files contain metadata for photos in the training set and test set, respectively. Each pet photo is labeled with the value of 1 (Yes) or 0 (No) for each of the following features:\n","\n","- Focus - Pet stands out against uncluttered background, not too close / far.\n","- Eyes - Both eyes are facing front or near-front, with at least 1 eye / pupil decently clear.\n","- Face - Decently clear face, facing front or near-front.\n","- Near - Single pet taking up significant portion of photo (roughly over 50% of photo width or height).\n","- Action - Pet in the middle of an action (e.g., jumping).\n","- Accessory - Accompanying physical or digital accessory / prop (i.e. toy, digital sticker), excluding collar and leash.\n","- Group - More than 1 pet in the photo.\n","- Collage - Digitally-retouched photo (i.e. with digital photo frame, combination of multiple photos).\n","- Human - Human in the photo.\n","- Occlusion - Specific undesirable objects blocking part of the pet (i.e. human, cage or fence). Note that not all blocking objects are considered occlusion.\n","- Info - Custom-added text or labels (i.e. pet name, description).\n","- Blur - Noticeably out of focus or noisy, especially for the pet’s eyes and face. For Blur entries, “Eyes” column is always set to 0."],"metadata":{}},{"cell_type":"markdown","source":["# Importing Libraries"],"metadata":{}},{"cell_type":"code","execution_count":2,"source":["import os\r\n","import pandas as pd\r\n","import numpy as np\r\n","import tensorflow as tf\r\n","from tensorflow import keras\r\n","from sklearn.model_selection import train_test_split\r\n","import matplotlib.pyplot as plt"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-27T10:57:33.851564Z","iopub.execute_input":"2021-09-27T10:57:33.853870Z","iopub.status.idle":"2021-09-27T10:57:38.797835Z","shell.execute_reply.started":"2021-09-27T10:57:33.853828Z","shell.execute_reply":"2021-09-27T10:57:38.797100Z"},"trusted":true}},{"cell_type":"code","execution_count":3,"source":["TRAIN_PATH = \"../input/petfinder-pawpularity-score/train/\"\r\n","TEST_PATH = \"../input/petfinder-pawpularity-score/test/\"\r\n","\r\n","IMAGE_SIZE = 224\r\n","BATCH_SIZE = 100\r\n","NO_OF_EPOCHS = 100"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-27T10:57:38.801089Z","iopub.execute_input":"2021-09-27T10:57:38.801300Z","iopub.status.idle":"2021-09-27T10:57:38.807793Z","shell.execute_reply.started":"2021-09-27T10:57:38.801269Z","shell.execute_reply":"2021-09-27T10:57:38.807054Z"},"trusted":true}},{"cell_type":"code","execution_count":4,"source":["train_df = pd.read_csv(\"../input/petfinder-pawpularity-score/train.csv\")\r\n","test_df = pd.read_csv(\"../input/petfinder-pawpularity-score/test.csv\")\r\n","\r\n","# Adding paths for images\r\n","train_df[\"Path\"] = TRAIN_PATH + train_df[\"Id\"] + \".jpg\"\r\n","test_df[\"Path\"] = TEST_PATH + test_df[\"Id\"] + \".jpg\"\r\n","train_df.head()"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-27T10:57:38.810598Z","iopub.execute_input":"2021-09-27T10:57:38.810822Z","iopub.status.idle":"2021-09-27T10:57:38.893957Z","shell.execute_reply.started":"2021-09-27T10:57:38.810799Z","shell.execute_reply":"2021-09-27T10:57:38.893292Z"},"trusted":true}},{"cell_type":"code","execution_count":5,"source":["test_df.head()"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-27T09:00:22.023176Z","iopub.execute_input":"2021-09-27T09:00:22.023501Z","iopub.status.idle":"2021-09-27T09:00:22.039984Z","shell.execute_reply.started":"2021-09-27T09:00:22.023467Z","shell.execute_reply":"2021-09-27T09:00:22.039330Z"},"trusted":true}},{"cell_type":"code","execution_count":6,"source":["print(\"Training set instances:\", train_df.shape[0])\r\n","print(\"Testing set instances:\", test_df.shape[0])"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-27T09:00:22.041159Z","iopub.execute_input":"2021-09-27T09:00:22.041600Z","iopub.status.idle":"2021-09-27T09:00:22.047170Z","shell.execute_reply.started":"2021-09-27T09:00:22.041567Z","shell.execute_reply":"2021-09-27T09:00:22.046234Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["# Creating keras Dataset"],"metadata":{}},{"cell_type":"code","execution_count":5,"source":["def generate_image_dataset(filepath, label=None):\r\n","    image = tf.io.read_file(filepath)\r\n","    image = tf.image.decode_jpeg(image, channels=3) \r\n","    iamge = tf.cast(image, tf.float32) / 255.0\r\n","    image = tf.image.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\r\n","    return image, label\r\n","\r\n","\r\n","train, valid = train_test_split(train_df, test_size=0.1)\r\n","dataset_train = (tf.data.Dataset.from_tensor_slices((train[\"Path\"].values,\r\n","                                                    train[\"Pawpularity\"].values )).\r\n","                 map(generate_image_dataset).batch(BATCH_SIZE).prefetch(1))\r\n","dataset_valid = (tf.data.Dataset.from_tensor_slices((valid[\"Path\"].values,\r\n","                                                    valid[\"Pawpularity\"].values)).\r\n","                map(generate_image_dataset).batch(BATCH_SIZE).prefetch(1))"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-27T10:57:38.895756Z","iopub.execute_input":"2021-09-27T10:57:38.895985Z","iopub.status.idle":"2021-09-27T10:57:40.530043Z","shell.execute_reply.started":"2021-09-27T10:57:38.895953Z","shell.execute_reply":"2021-09-27T10:57:40.529354Z"},"trusted":true}},{"cell_type":"code","execution_count":6,"source":["data_augmentation = keras.Sequential([\r\n","        keras.layers.experimental.preprocessing.Normalization(),\r\n","        keras.layers.experimental.preprocessing.Resizing(IMAGE_SIZE, IMAGE_SIZE),\r\n","        keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\r\n","        # random rotation in range [-20% * 2pi, 20% * 2pi]\r\n","        keras.layers.experimental.preprocessing.RandomRotation(factor=0.02), \r\n","        # A positive value means zooming out, while a negative value means zooming in.\r\n","        keras.layers.experimental.preprocessing.RandomZoom(\r\n","            height_factor=0.2, width_factor=0.2\r\n","        # output zoomed out vertically in range [20%, 20%]\r\n","        # output zoomed out horizontally in the range [20%, 20%]\r\n","        )\r\n","    ],\r\n","    name=\"data_augmentaion\"\r\n",")"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-27T10:58:06.319070Z","iopub.execute_input":"2021-09-27T10:58:06.319345Z","iopub.status.idle":"2021-09-27T10:58:06.647531Z","shell.execute_reply.started":"2021-09-27T10:58:06.319317Z","shell.execute_reply":"2021-09-27T10:58:06.646393Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["def train_experimental_model(model,\r\n","                           optimizer,\r\n","                           checkpoint_name=None):\r\n","    model.compile(\r\n","        optimizer=optimizer,\r\n","        loss=keras.losses.MeanSquaredError(),\r\n","        metrics=[\r\n","            keras.metrics.RootMeanSquaredError(name=\"rmse\"),\r\n","            \"mae\",\r\n","            \"mape\"\r\n","        ]\r\n","    )\r\n","    if checkpoint_name==None:\r\n","        raise ValueError(\"Provide checpoint model name.\")\r\n","    checkpoint_callback = keras.callbacks.ModelCheckpoint(\r\n","        checkpoint_name,\r\n","        monitor=\"val_rmse\",\r\n","        save_best_only=True,\r\n","        save_weights_only=True\r\n","    )\r\n","    early_stopping_callback = keras.callbacks.EarlyStopping(\r\n","        patience=10,\r\n","        restore_best_weights=True,\r\n","        min_delta=1e-4, \r\n","    )\r\n","    \r\n","    history = model.fit(\r\n","        dataset_train,\r\n","        validation_data=dataset_valid,\r\n","        batch_size=BATCH_SIZE,\r\n","        epochs=NO_OF_EPOCHS,\r\n","        callbacks=[checkpoint_callback, early_stopping_callback]\r\n","    )\r\n","    return history\r\n","\r\n","history = train_experimental_model(resnet_model,optimizer=keras.optimizers.Adam(),\r\n","                                   checkpoint_name=\"resnet_model.h5\")"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-27T10:58:11.593886Z","iopub.execute_input":"2021-09-27T10:58:11.594104Z"},"trusted":true}},{"cell_type":"markdown","source":["# Let's train a ResNet-34 CNN"],"metadata":{}},{"cell_type":"code","execution_count":7,"source":["class ResidualUnit(keras.layers.Layer):\r\n","    def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\r\n","        super().__init__(**kwargs)\r\n","        self.activation = keras.activations.get(activation)\r\n","        self.main_layers = [\r\n","            keras.layers.Conv2D(filters, 3, strides=strides, \r\n","                                padding=\"same\", use_bias=False),\r\n","            keras.layers.BatchNormalization(),\r\n","            self.activation,\r\n","            keras.layers.Conv2D(filters, 3, strides=1, padding=\"same\",\r\n","                               use_bias=False),\r\n","            keras.layers.BatchNormalization()\r\n","        ]\r\n","        self.skip_layers = []\r\n","        if strides > 1:\r\n","            self.skip_layers = [\r\n","                keras.layers.Conv2D(filters, 1, strides=strides,\r\n","                                    padding=\"same\", use_bias=False),\r\n","                keras.layers.BatchNormalization()\r\n","            ]\r\n","    def get_config(self):\r\n","        base_config = super().get_config()\r\n","        return {**base_config, \"activation\": self.activation,\r\n","                \"main_layers\": self.main_layers,\r\n","                \"skip_layers\": self.skip_layers}\r\n","\r\n","    def call(self, inputs):\r\n","        Z = inputs\r\n","        for layer in self.main_layers:\r\n","            Z = layer(Z)\r\n","        skip_Z = inputs\r\n","        for layer in self.skip_layers:\r\n","            skip_Z = layer(skip_Z)\r\n","        return self.activation(Z+skip_Z)\r\n","    \r\n","\r\n","resnet_model = keras.models.Sequential()\r\n","resnet_model.add(keras.layers.Conv2D(64, 7, strides=2, input_shape=[224, 224, 3],\r\n","                             padding=\"same\", use_bias=False))\r\n","resnet_model.add(data_augmentation)\r\n","resnet_model.add(keras.layers.BatchNormalization())\r\n","resnet_model.add(keras.layers.Activation(\"relu\"))\r\n","resnet_model.add(keras.layers.MaxPool2D(pool_size=2, strides=2, padding=\"same\"))\r\n","prev_filters = 64\r\n","for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\r\n","    strides = 1 if filters == prev_filters else 2\r\n","    resnet_model.add(ResidualUnit(filters, strides=strides))\r\n","    prev_filters = filters\r\n","resnet_model.add(keras.layers.GlobalAvgPool2D())\r\n","resnet_model.add(keras.layers.Flatten())\r\n","resnet_model.add(keras.layers.Dense(1))"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-27T10:58:10.622102Z","iopub.execute_input":"2021-09-27T10:58:10.624143Z","iopub.status.idle":"2021-09-27T10:58:11.592347Z","shell.execute_reply.started":"2021-09-27T10:58:10.624104Z","shell.execute_reply":"2021-09-27T10:58:11.591644Z"},"trusted":true}},{"cell_type":"markdown","source":["# Submission "],"metadata":{}},{"cell_type":"code","execution_count":11,"source":["def generate_test_dataset(filepath):\r\n","    image = tf.io.read_file(filepath)\r\n","    image = tf.image.decode_jpeg(image, channels=3) \r\n","    iamge = tf.cast(image, tf.float32) / 255.0\r\n","    image = tf.image.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\r\n","    return image, 0\r\n","\r\n","\r\n","dataset_test = (tf.data.Dataset.from_tensor_slices(test_df[\"Path\"].values).\r\n","                map(generate_test_dataset).batch(BATCH_SIZE).prefetch(1))\r\n","\r\n","submission_df = test_df[[\"Id\"]]\r\n","submission_df = submission_df.assign(Pawpularity=resnet_model.predict(dataset_test).reshape(-1))\r\n","submission_df.to_csv(\"submission.csv\", index=False)"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-27T09:48:41.262754Z","iopub.execute_input":"2021-09-27T09:48:41.263020Z","iopub.status.idle":"2021-09-27T09:48:41.920212Z","shell.execute_reply.started":"2021-09-27T09:48:41.262992Z","shell.execute_reply":"2021-09-27T09:48:41.919463Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{}}]}